# Performance Monitoring and Regression Detection
# Continuous performance monitoring with automated alerts and reporting

name: Performance Monitoring

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Weekly performance baseline update
    - cron: '0 4 * * 0'

jobs:
  # ==========================================================================
  # Performance Benchmarking
  # ==========================================================================
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]" pytest-benchmark asv hyperfine

      - name: Run pytest benchmarks
        run: |
          pytest tests/performance/ --benchmark-json=benchmark-results.json --benchmark-histogram=benchmark-histogram

      - name: Run ASV benchmarks
        run: |
          mkdir -p .asv
          asv machine --machine=github-actions
          asv run --machine=github-actions --python=3.11
          asv publish

      - name: Generate performance report
        run: |
          echo "# Performance Benchmark Report" > performance-report.md
          echo "Generated on: $(date)" >> performance-report.md
          echo "" >> performance-report.md
          
          # Extract key metrics from benchmark results
          python -c "
          import json
          with open('benchmark-results.json') as f:
              data = json.load(f)
          
          print('## Benchmark Summary', file=open('performance-report.md', 'a'))
          for benchmark in data['benchmarks']:
              name = benchmark['name']
              stats = benchmark['stats']
              mean = stats['mean']
              stddev = stats['stddev']
              print(f'- **{name}**: {mean:.4f}s Â± {stddev:.4f}s', file=open('performance-report.md', 'a'))
          "

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: |
            benchmark-results.json
            benchmark-histogram/
            performance-report.md
            .asv/

  # ==========================================================================
  # Memory Profiling
  # ==========================================================================
  memory-profile:
    name: Memory Profiling
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install profiling tools
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]" memory-profiler py-spy tracemalloc-tools

      - name: Run memory profiling
        run: |
          # Profile memory usage of core components
          python -m memory_profiler scripts/profile_memory.py > memory-profile.txt
          
          # Generate memory usage reports
          python -c "
          import tracemalloc
          import json
          from src.testgen_copilot.core import TestGenerator
          
          tracemalloc.start()
          
          # Simulate typical usage patterns
          generator = TestGenerator()
          # Add memory-intensive operations here
          
          current, peak = tracemalloc.get_traced_memory()
          tracemalloc.stop()
          
          memory_stats = {
              'current_mb': current / 1024 / 1024,
              'peak_mb': peak / 1024 / 1024
          }
          
          with open('memory-stats.json', 'w') as f:
              json.dump(memory_stats, f, indent=2)
          "

      - name: Check for memory leaks
        run: |
          # Run tests with memory leak detection
          python -c "
          import gc
          import psutil
          import os
          
          process = psutil.Process(os.getpid())
          initial_memory = process.memory_info().rss
          
          # Simulate operations that might leak memory
          for i in range(100):
              # Add test operations here
              gc.collect()
          
          final_memory = process.memory_info().rss
          memory_growth = (final_memory - initial_memory) / 1024 / 1024
          
          print(f'Memory growth: {memory_growth:.2f} MB')
          
          # Alert if memory growth is excessive
          if memory_growth > 10:  # 10 MB threshold
              print('WARNING: Potential memory leak detected!')
              exit(1)
          "

      - name: Upload memory profiling results
        uses: actions/upload-artifact@v3
        with:
          name: memory-profiling-results
          path: |
            memory-profile.txt
            memory-stats.json

  # ==========================================================================
  # Load Testing
  # ==========================================================================
  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install load testing tools
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]" locust httpx

      - name: Run load tests
        run: |
          # Start the application in background
          python -m testgen_copilot.cli --help > /dev/null 2>&1 &
          APP_PID=$!
          
          # Run load tests
          cd tests/load
          locust --headless --users 10 --spawn-rate 2 --run-time 60s --html load-test-report.html --csv load-test-results
          
          # Stop the application
          kill $APP_PID || true

      - name: Analyze load test results
        run: |
          cd tests/load
          python -c "
          import csv
          import json
          
          # Parse load test results
          results = []
          with open('load-test-results_stats.csv', 'r') as f:
              reader = csv.DictReader(f)
              for row in reader:
                  results.append(row)
          
          # Generate summary
          summary = {
              'total_requests': sum(int(r['Request Count']) for r in results if r['Request Count'].isdigit()),
              'failure_rate': sum(int(r['Failure Count']) for r in results if r['Failure Count'].isdigit()) / max(1, sum(int(r['Request Count']) for r in results if r['Request Count'].isdigit())),
              'avg_response_time': sum(float(r['Average Response Time']) for r in results if r['Average Response Time'].replace('.', '').isdigit()) / len(results)
          }
          
          with open('load-test-summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          "

      - name: Upload load test results
        uses: actions/upload-artifact@v3
        with:
          name: load-test-results
          path: |
            tests/load/load-test-report.html
            tests/load/load-test-results*.csv
            tests/load/load-test-summary.json

  # ==========================================================================
  # Performance Regression Detection
  # ==========================================================================
  regression-check:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [benchmark, memory-profile]
    if: github.event_name == 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download performance results
        uses: actions/download-artifact@v3
        with:
          name: performance-results

      - name: Check for performance regressions
        run: |
          python -c "
          import json
          import os
          
          # Load current benchmark results
          with open('benchmark-results.json') as f:
              current_results = json.load(f)
          
          # Download baseline results from main branch (if available)
          # This would typically be stored in a performance database
          
          # Simple regression detection logic
          regressions = []
          for benchmark in current_results['benchmarks']:
              name = benchmark['name']
              current_time = benchmark['stats']['mean']
              
              # Example: flag as regression if > 20% slower
              # In practice, you'd compare against historical baselines
              if current_time > 1.0:  # placeholder threshold
                  regressions.append({
                      'name': name,
                      'current_time': current_time,
                      'regression_percent': 0  # placeholder
                  })
          
          if regressions:
              print('Performance regressions detected:')
              for reg in regressions:
                  print(f'- {reg[\"name\"]}: {reg[\"current_time\"]:.4f}s')
              
              # Create regression report
              with open('regression-report.json', 'w') as f:
                  json.dump(regressions, f, indent=2)
          else:
              print('No performance regressions detected')
          "

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            let comment = '## ðŸ“Š Performance Analysis Results\n\n';
            
            // Read performance report
            if (fs.existsSync('performance-report.md')) {
              const report = fs.readFileSync('performance-report.md', 'utf8');
              comment += report + '\n';
            }
            
            // Check for regressions
            if (fs.existsSync('regression-report.json')) {
              const regressions = JSON.parse(fs.readFileSync('regression-report.json', 'utf8'));
              comment += '### âš ï¸ Performance Regressions Detected\n\n';
              regressions.forEach(reg => {
                comment += `- **${reg.name}**: ${reg.current_time.toFixed(4)}s\n`;
              });
            } else {
              comment += '### âœ… No Performance Regressions Detected\n';
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # ==========================================================================
  # Performance Monitoring Dashboard
  # ==========================================================================
  dashboard-update:
    name: Update Performance Dashboard
    runs-on: ubuntu-latest
    needs: [benchmark, memory-profile, load-test]
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all performance artifacts
        uses: actions/download-artifact@v3

      - name: Generate performance dashboard data
        run: |
          python -c "
          import json
          import datetime
          from pathlib import Path
          
          # Consolidate all performance data
          dashboard_data = {
              'timestamp': datetime.datetime.utcnow().isoformat(),
              'commit_sha': '${{ github.sha }}',
              'benchmarks': {},
              'memory_stats': {},
              'load_test_results': {}
          }
          
          # Load benchmark results
          if Path('benchmark-results.json').exists():
              with open('benchmark-results.json') as f:
                  benchmark_data = json.load(f)
              dashboard_data['benchmarks'] = benchmark_data
          
          # Load memory stats
          if Path('memory-stats.json').exists():
              with open('memory-stats.json') as f:
                  memory_data = json.load(f)
              dashboard_data['memory_stats'] = memory_data
          
          # Load load test summary
          if Path('load-test-summary.json').exists():
              with open('load-test-summary.json') as f:
                  load_data = json.load(f)
              dashboard_data['load_test_results'] = load_data
          
          # Save dashboard data
          with open('performance-dashboard-data.json', 'w') as f:
              json.dump(dashboard_data, f, indent=2)
          "

      - name: Upload dashboard data
        uses: actions/upload-artifact@v3
        with:
          name: performance-dashboard-data
          path: performance-dashboard-data.json