# Monitoring and Observability

This document outlines the comprehensive monitoring and observability strategy for TestGen-Copilot.

## Overview

TestGen-Copilot uses a modern observability stack based on:
- **Prometheus**: Metrics collection and alerting
- **Grafana**: Metrics visualization and dashboards
- **OpenTelemetry**: Distributed tracing and metrics
- **Structured Logging**: JSON-formatted logs for analysis
- **Health Checks**: Application and dependency health monitoring

## Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Application   │────│   Prometheus    │────│     Grafana     │
│   (Metrics)     │    │   (Collection)  │    │  (Visualization)│
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         │              ┌─────────────────┐    ┌─────────────────┐
         │              │  Alertmanager   │    │   Log Aggreg.   │
         │              │   (Alerting)    │    │   (ELK/Fluentd) │
         └──────────────┴─────────────────┴────┴─────────────────┘
```

## Metrics Collection

### Application Metrics

The application exposes metrics on `/metrics` endpoint:

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# Request metrics
request_count = Counter('testgen_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
request_duration = Histogram('testgen_request_duration_seconds', 'Request duration')
active_requests = Gauge('testgen_active_requests', 'Active requests')

# Business metrics
tests_generated = Counter('testgen_tests_generated_total', 'Total tests generated', ['language'])
security_issues = Counter('testgen_security_issues_total', 'Security issues found', ['severity'])
llm_requests = Counter('testgen_llm_requests_total', 'LLM API requests', ['provider', 'status'])
llm_tokens = Counter('testgen_llm_tokens_total', 'LLM tokens used', ['provider', 'type'])
```

### System Metrics

Collected via node_exporter and cAdvisor:
- CPU usage and load
- Memory usage and allocation
- Disk I/O and space
- Network traffic
- Container metrics

### Custom Metrics

```python
# Performance metrics
generation_time = Histogram('testgen_generation_duration_seconds', 'Test generation time', ['language'])
file_size = Histogram('testgen_file_size_bytes', 'Source file size processed')
test_quality = Histogram('testgen_test_quality_score', 'Generated test quality score')

# Cache metrics  
cache_hits = Counter('testgen_cache_hits_total', 'Cache hits')
cache_misses = Counter('testgen_cache_misses_total', 'Cache misses')
cache_size = Gauge('testgen_cache_size_bytes', 'Cache size in bytes')

# Error metrics
errors_total = Counter('testgen_errors_total', 'Total errors', ['type', 'component'])
llm_errors = Counter('testgen_llm_errors_total', 'LLM API errors', ['provider', 'error_type'])
```

## Dashboards

### Main Dashboard (testgen-copilot-overview.json)

**Application Health**
- Service uptime and availability
- Request rate and error rate
- Response time percentiles
- Active users and sessions

**Performance Metrics**
- Test generation throughput
- Average generation time
- Queue length and processing time
- Resource utilization

**Business Metrics**
- Tests generated by language
- Security issues detected
- API usage and costs
- User engagement metrics

**Infrastructure**
- CPU and memory usage
- Disk and network I/O
- Container health
- Database performance

### Additional Dashboards

**Security Dashboard**
- Vulnerability detection rates
- Security issue types and severity
- False positive rates
- Scan coverage metrics

**LLM Usage Dashboard**
- API request rates by provider
- Token usage and costs
- Response times and error rates
- Model performance comparison

**User Experience Dashboard**
- User journey metrics
- Feature adoption rates
- Error rates by feature
- Performance impact on UX

## Alerting

### Critical Alerts (Immediate Response)

```yaml
# Service Down
- alert: TestGenServiceDown
  expr: up{job="testgen-copilot"} == 0
  for: 1m
  severity: critical
  summary: TestGen service is down

# High Error Rate
- alert: HighErrorRate
  expr: rate(testgen_errors_total[5m]) > 0.1
  for: 2m
  severity: critical
  summary: High error rate detected

# Memory Usage Critical
- alert: HighMemoryUsage
  expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
  for: 5m
  severity: critical
  summary: Memory usage above 90%
```

### Warning Alerts (Investigation Required)

```yaml
# Slow Response Time
- alert: SlowResponseTime
  expr: histogram_quantile(0.95, rate(testgen_request_duration_seconds_bucket[5m])) > 2
  for: 5m
  severity: warning
  summary: 95th percentile response time above 2s

# LLM API Errors
- alert: LLMAPIErrors
  expr: rate(testgen_llm_errors_total[10m]) > 0.05
  for: 3m
  severity: warning
  summary: High LLM API error rate

# Cache Miss Rate
- alert: HighCacheMissRate
  expr: rate(testgen_cache_misses_total[10m]) / rate(testgen_cache_hits_total[10m]) > 0.5
  for: 10m
  severity: warning
  summary: High cache miss rate
```

### Notification Channels

**Critical Alerts**
- PagerDuty/OpsGenie for immediate escalation
- Slack #alerts channel
- SMS for on-call engineers

**Warning Alerts**
- Slack #monitoring channel
- Email notifications
- JIRA ticket creation for persistent issues

## Health Checks

### Application Health Endpoints

```python
@app.route('/health')
def health_check():
    """Basic health check - service is running"""
    return {'status': 'healthy', 'timestamp': time.time()}

@app.route('/ready')
def readiness_check():
    """Readiness check - service can handle requests"""
    checks = {
        'database': check_database(),
        'redis': check_redis(),
        'llm_api': check_llm_connectivity(),
        'disk_space': check_disk_space()
    }
    
    all_healthy = all(checks.values())
    status_code = 200 if all_healthy else 503
    
    return {'status': 'ready' if all_healthy else 'not ready', 
            'checks': checks}, status_code

@app.route('/alive')
def liveness_check():
    """Liveness check - service should be restarted if this fails"""
    try:
        # Check critical components only
        return {'status': 'alive', 'timestamp': time.time()}
    except Exception as e:
        return {'status': 'dead', 'error': str(e)}, 500
```

### Dependency Health Checks

```python
def check_database():
    try:
        # Simple database connectivity test
        db.execute('SELECT 1')
        return True
    except Exception:
        return False

def check_redis():
    try:
        redis_client.ping()
        return True
    except Exception:
        return False

def check_llm_connectivity():
    try:
        # Test API connectivity with minimal request
        response = llm_client.test_connection()
        return response.status_code == 200
    except Exception:
        return False
```

## Logging Strategy

### Structured Logging

```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, name):
        self.logger = logging.getLogger(name)
        
    def log(self, level, message, **kwargs):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': level,
            'message': message,
            'service': 'testgen-copilot',
            **kwargs
        }
        self.logger.log(getattr(logging, level), json.dumps(log_entry))

# Usage examples
logger = StructuredLogger('testgen')

logger.log('INFO', 'Test generation started', 
          user_id='123', file_path='src/calc.py', language='python')

logger.log('ERROR', 'LLM API request failed',
          error_type='timeout', provider='openai', retry_count=3)

logger.log('DEBUG', 'Cache hit',
          cache_key='test_cache:abc123', ttl=3600)
```

### Log Levels and Usage

**DEBUG**: Detailed information for debugging
- Cache operations
- Internal state changes
- Performance measurements

**INFO**: General operational information
- Request processing
- User actions
- Business events

**WARNING**: Potential issues that don't affect functionality
- Slow operations
- Deprecated API usage
- Resource usage warnings

**ERROR**: Errors that affect functionality
- API failures
- Processing errors
- Configuration issues

**CRITICAL**: Errors that may cause service failure
- Database connectivity loss
- Critical resource exhaustion
- Security incidents

### Log Aggregation

**Development**: Console output with pretty formatting
```python
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

**Production**: JSON format for aggregation
```python
import json_logging

json_logging.init_non_web(enable_json=True)
logging.basicConfig(level=logging.INFO)
```

## Tracing

### OpenTelemetry Integration

```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Configure tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=14268,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Usage in application
@tracer.start_as_current_span("generate_tests")
def generate_tests(file_path: str):
    span = trace.get_current_span()
    span.set_attribute("file.path", file_path)
    span.set_attribute("file.language", detect_language(file_path))
    
    try:
        # Test generation logic
        result = perform_generation(file_path)
        span.set_attribute("tests.count", len(result))
        span.set_status(trace.Status(trace.StatusCode.OK))
        return result
    except Exception as e:
        span.record_exception(e)
        span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
        raise
```

### Distributed Tracing

Track requests across services:
- API Gateway → TestGen Service
- TestGen Service → LLM API
- TestGen Service → Cache/Database
- Service → Background Tasks

## Performance Monitoring

### Key Performance Indicators (KPIs)

**Availability**
- Service uptime: >99.9%
- Error rate: <0.1%
- Response time (95th percentile): <2s

**Performance**
- Test generation time: <30s for typical files
- Throughput: >100 requests/minute
- Cache hit rate: >80%

**Business Metrics**
- Tests generated per day
- Security issues detected
- API cost efficiency
- User satisfaction score

### SLA Monitoring

```python
# SLA tracking metrics
sla_availability = Gauge('testgen_sla_availability_percent', 'Service availability SLA')
sla_response_time = Gauge('testgen_sla_response_time_seconds', 'Response time SLA')
sla_error_rate = Gauge('testgen_sla_error_rate_percent', 'Error rate SLA')

def update_sla_metrics():
    # Calculate and update SLA metrics
    availability = calculate_availability()
    response_time = calculate_p95_response_time()
    error_rate = calculate_error_rate()
    
    sla_availability.set(availability)
    sla_response_time.set(response_time)
    sla_error_rate.set(error_rate)
```

## Operational Procedures

### Daily Monitoring Checklist

1. **Service Health**
   - [ ] Check service uptime in Grafana
   - [ ] Review error rates and alerts
   - [ ] Verify all health checks passing

2. **Performance Review**
   - [ ] Check response time trends
   - [ ] Review resource utilization
   - [ ] Monitor cache performance

3. **Business Metrics**
   - [ ] Review test generation volume
   - [ ] Check LLM API usage and costs
   - [ ] Monitor user engagement

4. **Security Monitoring**
   - [ ] Review security scan results
   - [ ] Check for unusual access patterns
   - [ ] Monitor failed authentication attempts

### Weekly Review Process

1. **Trend Analysis**
   - Performance trends over the week
   - Resource usage patterns
   - Error rate analysis

2. **Capacity Planning**
   - Resource utilization review
   - Growth trend analysis
   - Scaling recommendations

3. **Alert Review**
   - False positive analysis
   - Alert threshold tuning
   - Response time evaluation

## Troubleshooting

### Common Issues and Solutions

**High Memory Usage**
```bash
# Check memory usage by component
kubectl top pods
docker stats

# Analyze memory leaks
python -c "import psutil; print(psutil.virtual_memory())"

# Check for memory-intensive operations
grep "memory" /var/log/testgen/*.log
```

**Slow Response Times**
```bash
# Check active requests
curl http://localhost:8000/metrics | grep active_requests

# Analyze slow queries
tail -f /var/log/testgen/app.log | grep "duration > 1000"

# Check resource contention
iotop -a
```

**LLM API Issues**
```bash
# Check API connectivity
curl -v https://api.openai.com/v1/models

# Review API error logs
grep "llm_error" /var/log/testgen/*.log | tail -20

# Check rate limiting
curl http://localhost:8000/metrics | grep llm_requests
```

### Debug Mode

Enable detailed monitoring for troubleshooting:

```python
# Enable debug metrics
DEBUG_METRICS = True

if DEBUG_METRICS:
    # Additional detailed metrics
    request_size = Histogram('testgen_request_size_bytes', 'Request size')
    response_size = Histogram('testgen_response_size_bytes', 'Response size')
    queue_wait_time = Histogram('testgen_queue_wait_seconds', 'Queue wait time')
```

## Integration with External Systems

### ELK Stack Integration
```yaml
# Filebeat configuration
filebeat.inputs:
- type: log
  paths:
    - /var/log/testgen/*.log
  json.keys_under_root: true
  json.add_error_key: true

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "testgen-logs-%{+yyyy.MM.dd}"
```

### DataDog Integration
```python
from datadog import DogStatsdClient

statsd = DogStatsdClient(host='localhost', port=8125)

# Send custom metrics
statsd.increment('testgen.requests', tags=['endpoint:generate'])
statsd.histogram('testgen.response_time', response_time, tags=['language:python'])
```

### New Relic Integration
```python
import newrelic.agent

@newrelic.agent.function_trace()
def generate_tests(file_path):
    # Function will be traced automatically
    pass

# Custom metrics
newrelic.agent.record_custom_metric('Custom/TestsGenerated', count)
```

---

For additional monitoring questions, see the [OPERATIONAL_RUNBOOKS.md](../runbooks/) or contact the DevOps team.