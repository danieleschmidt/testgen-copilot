# Alertmanager Configuration for TestGen Copilot Assistant
# Advanced alerting with intelligent routing and escalation

global:
  # Global SMTP configuration
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@testgen.dev'
  smtp_auth_username: 'alerts@testgen.dev'
  smtp_auth_password: 'password'
  
  # Global Slack configuration
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

# Alert routing tree
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: critical-alerts
      group_wait: 0s
      repeat_interval: 15m
      routes:
        - match:
            alertname: ServiceDown
          receiver: service-down-pager
        - match:
            alertname: HighErrorRate
          receiver: error-rate-alerts
    
    # Warning alerts - standard notification
    - match:
        severity: warning
      receiver: warning-alerts
      repeat_interval: 2h
    
    # Performance alerts
    - match:
        category: performance
      receiver: performance-alerts
      repeat_interval: 30m
    
    # Security alerts - special handling
    - match:
        category: security
      receiver: security-alerts
      group_wait: 0s
      repeat_interval: 5m

# Inhibition rules to reduce noise
inhibit_rules:
  # Inhibit warning if critical is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
  
  # Inhibit individual service alerts if overall health is down
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '(HighMemoryUsage|HighCPUUsage|DiskSpaceRunningLow)'
    equal: ['instance']

receivers:
  # Default webhook receiver
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://localhost:5001/webhook'
        send_resolved: true

  # Critical alerts - multiple channels
  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@testgen.dev'
        from: 'alerts@testgen.dev'
        subject: 'üö® CRITICAL: {{ .GroupLabels.alertname }} - {{ .GroupLabels.instance }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Severity: {{ .Labels.severity }}
          Time: {{ .StartsAt }}
          {{ end }}
        headers:
          X-Priority: '1'
    
    slack_configs:
      - channel: '#alerts-critical'
        color: 'danger'
        title: 'üö® Critical Alert'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Instance:* {{ .Labels.instance }}
          *Severity:* {{ .Labels.severity }}
          {{ end }}
        actions:
          - type: button
            text: 'View Grafana Dashboard'
            url: 'http://grafana.testgen.local/d/overview'
          - type: button
            text: 'View Logs'
            url: 'http://kibana.testgen.local'
    
    pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_ROUTING_KEY'
        description: '{{ .GroupLabels.alertname }} on {{ .GroupLabels.instance }}'
        severity: 'critical'

  # Service down specific handling
  - name: 'service-down-pager'
    pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_ROUTING_KEY'
        description: 'TestGen Copilot service is down on {{ .GroupLabels.instance }}'
        severity: 'critical'
        client: 'Alertmanager'
        client_url: 'http://alertmanager.testgen.local'
    
    slack_configs:
      - channel: '#incidents'
        color: 'danger'
        title: 'üö® SERVICE DOWN'
        text: |
          TestGen Copilot Assistant is down!
          Instance: {{ .GroupLabels.instance }}
          Time: {{ .CommonAnnotations.startsAt }}
        actions:
          - type: button
            text: 'Emergency Runbook'
            url: 'https://docs.testgen.dev/runbooks/service-down'
          - type: button
            text: 'Incident Response'
            url: 'https://incident.testgen.dev/new'

  # Error rate alerts
  - name: 'error-rate-alerts'
    email_configs:
      - to: 'team@testgen.dev'
        subject: '‚ö†Ô∏è High Error Rate Detected'
        body: |
          High error rate detected in TestGen Copilot Assistant.
          
          Current error rate: {{ .CommonAnnotations.value }}%
          Threshold: {{ .CommonAnnotations.threshold }}%
          Duration: {{ .CommonAnnotations.duration }}
          
          Please investigate immediately.
    
    slack_configs:
      - channel: '#alerts'
        color: 'warning'
        title: '‚ö†Ô∏è High Error Rate'
        text: |
          Error rate: {{ .CommonAnnotations.value }}% (threshold: {{ .CommonAnnotations.threshold }}%)
          Duration: {{ .CommonAnnotations.duration }}

  # Warning alerts
  - name: 'warning-alerts'
    email_configs:
      - to: 'team@testgen.dev'
        subject: '‚ö†Ô∏è Warning: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Warning: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          {{ end }}
    
    slack_configs:
      - channel: '#monitoring'
        color: 'warning'
        title: '‚ö†Ô∏è Warning Alert'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          Instance: {{ .Labels.instance }}
          {{ end }}

  # Performance alerts
  - name: 'performance-alerts'
    slack_configs:
      - channel: '#performance'
        color: 'warning'
        title: 'üìä Performance Alert'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          Value: {{ .Annotations.value }}
          Instance: {{ .Labels.instance }}
          {{ end }}
        actions:
          - type: button
            text: 'Performance Dashboard'
            url: 'http://grafana.testgen.local/d/performance'

  # Security alerts
  - name: 'security-alerts'
    email_configs:
      - to: 'security@testgen.dev'
        subject: 'üîí SECURITY ALERT: {{ .GroupLabels.alertname }}'
        body: |
          SECURITY ALERT DETECTED
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Severity: {{ .Labels.severity }}
          {{ end }}
          
          Please investigate immediately and follow security incident response procedures.
        headers:
          X-Priority: '1'
          X-Security-Alert: 'true'
    
    slack_configs:
      - channel: '#security-alerts'
        color: 'danger'
        title: 'üîí Security Alert'
        text: |
          {{ range .Alerts }}
          *SECURITY ALERT:* {{ .Annotations.summary }}
          Instance: {{ .Labels.instance }}
          Severity: {{ .Labels.severity }}
          {{ end }}
        actions:
          - type: button
            text: 'Security Runbook'
            url: 'https://docs.testgen.dev/runbooks/security-incident'
          - type: button
            text: 'Create Incident'
            url: 'https://security.testgen.dev/incidents/new'
    
    webhook_configs:
      - url: 'https://security.testgen.dev/api/alerts'
        send_resolved: true
        http_config:
          bearer_token: 'your-security-api-token'

# Template definitions for custom message formatting
templates:
  - '/etc/alertmanager/templates/*.tmpl'