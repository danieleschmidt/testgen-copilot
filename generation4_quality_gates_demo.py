"""Generation 4: Quality Gates - Demonstration

This script demonstrates the comprehensive quality assurance capabilities 
implemented in Generation 4, including automated testing, code quality analysis,
security validation, and deployment readiness checks.
"""

import asyncio
import sys
import time
from pathlib import Path

# Add src to path
sys.path.insert(0, 'src')

from testgen_copilot.quality_gates_system import (
    QualityGatesSystem, QualityGateStatus, QualityMetric, TestType
)

async def demonstrate_generation4_quality_gates():
    """Demonstrate Generation 4 quality gates capabilities."""
    
    print("üéØ Generation 4: Quality Gates - COMPREHENSIVE DEMONSTRATION")
    print("=" * 70)
    
    # 1. Initialize quality gates system
    print("\n1. Initializing Quality Gates System...")
    system = QualityGatesSystem(
        parallel_execution=True,
        max_workers=4
    )
    
    print(f"‚úÖ Quality Gates System initialized:")
    print(f"   - Total quality gates: {len(system.quality_gates)}")
    print(f"   - Parallel execution: {system.parallel_execution}")
    print(f"   - Max workers: {system.max_workers}")
    print(f"   - Quality thresholds configured: {len(system.quality_thresholds)}")
    
    # Display configured gates
    print("\n   üìã Configured Quality Gates:")
    for i, gate in enumerate(system.quality_gates, 1):
        blocking_indicator = "üö´ BLOCKING" if gate.blocking else "‚ö†Ô∏è  NON-BLOCKING"
        print(f"      {i}. {gate.name} - {blocking_indicator}")
        print(f"         {gate.description}")
        print(f"         Timeout: {gate.timeout_seconds}s, Requirements: {len(gate.requirements)}")
    
    # 2. Configure custom quality thresholds
    print("\n2. Configuring Custom Quality Thresholds...")
    custom_config = {
        "quality_thresholds": {
            "code_coverage": 85.0,
            "cyclomatic_complexity": 8.0,
            "maintainability_index": 65.0,
            "security_hotspots": 0,
            "performance_score": 82.0
        },
        "gate_configuration": {
            "performance_benchmark": {
                "blocking": False,  # Make performance non-blocking for demo
                "timeout_seconds": 180
            },
            "documentation": {
                "blocking": False,
                "timeout_seconds": 90
            }
        }
    }
    
    print("‚úÖ Custom configuration prepared:")
    for metric, threshold in custom_config["quality_thresholds"].items():
        print(f"   - {metric.replace('_', ' ').title()}: {threshold}")
    
    # 3. Execute comprehensive quality pipeline
    print("\n3. Executing Comprehensive Quality Pipeline...")
    project_path = Path.cwd()
    
    start_time = time.time()
    
    try:
        # Execute the full quality pipeline
        quality_report = await system.execute_quality_pipeline(
            project_path=project_path,
            skip_gates=[],  # Don't skip any gates
            custom_config=custom_config
        )
        
        pipeline_duration = time.time() - start_time
        
        print(f"\n‚úÖ Quality pipeline execution completed in {pipeline_duration:.2f} seconds")
        
        # 4. Display detailed results
        print("\n4. Quality Pipeline Results Analysis...")
        
        # Overall status
        status_icon = {
            QualityGateStatus.PASSED: "‚úÖ",
            QualityGateStatus.WARNING: "‚ö†Ô∏è",
            QualityGateStatus.FAILED: "‚ùå"
        }.get(quality_report.overall_status, "‚ùì")
        
        print(f"   üéØ Overall Status: {status_icon} {quality_report.overall_status.value.upper()}")
        print(f"   üìä Gates Summary:")
        print(f"      - Passed: {quality_report.gates_passed}")
        print(f"      - Failed: {quality_report.gates_failed}")
        print(f"      - Warnings: {quality_report.gates_warnings}")
        print(f"      - Total: {quality_report.gates_passed + quality_report.gates_failed + quality_report.gates_warnings}")
        
        # Individual gate results
        print(f"\n   üîç Individual Gate Results:")
        for gate in system.quality_gates:
            status_icon = {
                QualityGateStatus.PASSED: "‚úÖ",
                QualityGateStatus.WARNING: "‚ö†Ô∏è",
                QualityGateStatus.FAILED: "‚ùå",
                QualityGateStatus.SKIPPED: "‚è≠Ô∏è",
                QualityGateStatus.PENDING: "‚è≥"
            }.get(gate.status, "‚ùì")
            
            duration = "N/A"
            if gate.start_time and gate.end_time:
                duration = f"{(gate.end_time - gate.start_time).total_seconds():.1f}s"
            
            print(f"      {status_icon} {gate.name}: {gate.status.value} ({duration})")
            
            if gate.error_message:
                print(f"         Error: {gate.error_message}")
            elif gate.results and gate.status != QualityGateStatus.PENDING:
                # Show key results
                if gate.gate_type == "analysis":
                    if "code_coverage" in gate.results:
                        print(f"         Coverage: {gate.results['code_coverage']:.1f}%")
                    if "complexity" in gate.results:
                        complexity = gate.results["complexity"]
                        if "average_complexity" in complexity:
                            print(f"         Avg Complexity: {complexity['average_complexity']:.1f}")
                elif gate.gate_type == "testing":
                    total = gate.results.get("total_tests", 0)
                    passed = gate.results.get("passed_tests", 0)
                    print(f"         Tests: {passed}/{total} passed ({gate.results.get('pass_rate', 0):.1f}%)")
                elif gate.gate_type == "security":
                    findings = gate.results.get("total_findings", 0)
                    critical = gate.results.get("critical_findings", 0)
                    print(f"         Security: {findings} findings ({critical} critical)")
                elif gate.gate_type == "performance":
                    score = gate.results.get("performance_score", 0)
                    print(f"         Performance Score: {score:.1f}/100")
        
        # 5. Quality metrics analysis
        print(f"\n5. Quality Metrics Analysis...")
        if quality_report.quality_metrics:
            print(f"   üìà Key Quality Metrics:")
            for metric, value in quality_report.quality_metrics.items():
                threshold = system.quality_thresholds.get(metric, "N/A")
                
                # Determine if metric meets threshold
                meets_threshold = "‚úÖ"
                if threshold != "N/A":
                    if metric in [QualityMetric.CYCLOMATIC_COMPLEXITY, QualityMetric.SECURITY_HOTSPOTS]:
                        meets_threshold = "‚úÖ" if value <= threshold else "‚ùå"
                    else:
                        meets_threshold = "‚úÖ" if value >= threshold else "‚ùå"
                
                print(f"      {meets_threshold} {metric.value.replace('_', ' ').title()}: {value:.1f} (threshold: {threshold})")
        
        # 6. Test results summary
        print(f"\n6. Test Results Summary...")
        if quality_report.test_results:
            test_types = {}
            for test_result in quality_report.test_results:
                test_type = test_result.test_type.value
                if test_type not in test_types:
                    test_types[test_type] = {"total": 0, "passed": 0, "failed": 0, "time": 0.0}
                
                test_types[test_type]["total"] += 1
                if test_result.status == QualityGateStatus.PASSED:
                    test_types[test_type]["passed"] += 1
                else:
                    test_types[test_type]["failed"] += 1
                test_types[test_type]["time"] += test_result.execution_time
            
            print(f"   üß™ Test Execution Summary:")
            total_tests = sum(t["total"] for t in test_types.values())
            total_passed = sum(t["passed"] for t in test_types.values())
            overall_pass_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
            
            print(f"      Overall: {total_passed}/{total_tests} tests passed ({overall_pass_rate:.1f}%)")
            
            for test_type, stats in test_types.items():
                pass_rate = (stats["passed"] / stats["total"] * 100) if stats["total"] > 0 else 0
                avg_time = stats["time"] / stats["total"] if stats["total"] > 0 else 0
                print(f"      {test_type.title()}: {stats['passed']}/{stats['total']} passed ({pass_rate:.1f}%, avg: {avg_time:.3f}s)")
        
        # 7. Security findings
        print(f"\n7. Security Assessment...")
        if quality_report.security_findings:
            print(f"   üîí Security Findings: {len(quality_report.security_findings)}")
            
            # Group by severity
            severity_counts = {}
            for finding in quality_report.security_findings:
                severity = finding.get("severity", "UNKNOWN")
                severity_counts[severity] = severity_counts.get(severity, 0) + 1
            
            for severity, count in severity_counts.items():
                severity_icon = {"CRITICAL": "üî¥", "HIGH": "üü†", "MEDIUM": "üü°", "LOW": "üü¢"}.get(severity, "‚ö™")
                print(f"      {severity_icon} {severity}: {count} findings")
            
            # Show sample findings
            if len(quality_report.security_findings) > 0:
                print(f"   üîç Sample Security Finding:")
                sample = quality_report.security_findings[0]
                print(f"      Package: {sample.get('package', 'N/A')}")
                print(f"      Vulnerability: {sample.get('vulnerability', 'N/A')}")
                print(f"      Severity: {sample.get('severity', 'N/A')}")
                print(f"      Description: {sample.get('description', 'N/A')}")
        else:
            print(f"   üîí Security Assessment: ‚úÖ No security issues found")
        
        # 8. Performance benchmarks
        print(f"\n8. Performance Benchmarks...")
        if quality_report.performance_benchmarks:
            print(f"   ‚ö° Performance Metrics:")
            for metric, value in quality_report.performance_benchmarks.items():
                if "response_time" in metric:
                    print(f"      üìä {metric.replace('_', ' ').title()}: {value:.3f}s")
                elif "throughput" in metric:
                    print(f"      üìä {metric.replace('_', ' ').title()}: {value:.1f} RPS")
                elif "memory" in metric:
                    print(f"      üìä {metric.replace('_', ' ').title()}: {value:.1f} MB")
                elif "score" in metric:
                    print(f"      üìä {metric.replace('_', ' ').title()}: {value:.1f}/100")
                else:
                    print(f"      üìä {metric.replace('_', ' ').title()}: {value}")
        
        # 9. Recommendations
        print(f"\n9. Quality Improvement Recommendations...")
        if quality_report.recommendations:
            print(f"   üí° Top Recommendations:")
            for i, recommendation in enumerate(quality_report.recommendations[:5], 1):
                print(f"      {i}. {recommendation}")
            
            if len(quality_report.recommendations) > 5:
                print(f"      ... and {len(quality_report.recommendations) - 5} more recommendations")
        else:
            print(f"   üí° No specific recommendations - quality targets met!")
        
        # 10. Generate and display dashboard
        print(f"\n10. Generating Quality Dashboard...")
        await system.generate_quality_dashboard()
        
        # 11. Export quality report
        print(f"\n11. Exporting Quality Report...")
        report_path = await system.export_quality_report(Path(f"quality_report_{int(time.time())}.json"))
        print(f"    üìÑ Quality report exported to: {report_path}")
        
        # 12. Final summary
        print(f"\n12. Quality Gates Summary...")
        execution_time = (quality_report.end_time - quality_report.start_time).total_seconds()
        
        print(f"    üéØ Pipeline ID: {quality_report.pipeline_id}")
        print(f"    ‚è±Ô∏è  Execution Time: {execution_time:.2f} seconds")
        print(f"    üìä Overall Status: {quality_report.overall_status.value.upper()}")
        print(f"    ‚úÖ Success Rate: {quality_report.gates_passed}/{quality_report.gates_passed + quality_report.gates_failed} gates passed")
        
        if quality_report.overall_status == QualityGateStatus.PASSED:
            print(f"    üéâ All critical quality gates passed - Ready for deployment!")
        elif quality_report.overall_status == QualityGateStatus.WARNING:
            print(f"    ‚ö†Ô∏è  Quality gates passed with warnings - Review recommendations")
        else:
            print(f"    ‚ùå Quality gates failed - Address critical issues before deployment")
        
        # Clean up
        system.cleanup()
        
    except Exception as e:
        print(f"‚ùå Quality pipeline execution failed: {e}")
        system.cleanup()
        raise
    
    print("\nüéØ Generation 4: Quality Gates - DEMONSTRATION COMPLETE!")
    print("   üéØ Multi-stage quality gates: 7 comprehensive validation stages")
    print("   üß™ Automated testing: Unit, integration, and functional tests")
    print("   üìä Code quality analysis: Coverage, complexity, maintainability")
    print("   üîí Security validation: Vulnerability scanning and threat detection")
    print("   ‚ö° Performance benchmarking: Load testing and response time analysis")
    print("   üìÑ Comprehensive reporting: Detailed metrics and recommendations")
    print("   üöÄ Deployment readiness: Automated go/no-go decision making")


if __name__ == "__main__":
    asyncio.run(demonstrate_generation4_quality_gates())